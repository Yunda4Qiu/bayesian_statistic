{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbVuVPrKgYT3"
   },
   "source": [
    "# Weekly assignment 2\n",
    "\n",
    "## Read before you start\n",
    "\n",
    "* Provide clear and complete answers in code blocks or markdown. You may add as many as you need.\n",
    "* Always motivate your answers. This can be done in markdown cells, or in comments in code.\n",
    "* Submit your results via Brightspace. Use the following filename convention: ``StudentName1_snumber1_StudentName2_snumber2_Assignment2.ipynb``.\n",
    "* Make sure you submit a fully executed version of the notebook file. The teaching assistants will not run/debug your code during grading.\n",
    "* Questions? Ask them during the workgroups, or see Brightspace for instructions on how to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6S69FFhgYT5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Belief updating\n",
    "\n",
    "A researcher is doing a study into voting behaviour. They conduct a poll and ask people on the street whether they have voted at the latest elections.\n",
    "\n",
    "After a day of collecting data, the researcher finds that $z=113$ out of $N=145$ people (claimed to) vote. These data inform the _likelihood_, which is a binomial distribution (Note: the product of Bernoulli's as shown in the lecture is very similar to the binomial distribution. The binomial distribution does not care about the _order_ in which people voted yes/no, which implies the binomial coefficient $\\binom{N}{z}$):\n",
    "\n",
    "$$ p(z\\mid N, \\theta) = \\binom{N}{z} \\theta^z (1-\\theta)^{N-z} $$\n",
    "\n",
    "Assume that the researcher started with a completely uniform _prior belief_ over the voting probability $\\theta$. They represent this belief with a beta distribution (see the slides from Lecture 2).\n",
    "\n",
    "1. What settings for $a$ and $b$ are appropriate here?\n",
    "\n",
    "The researcher can easily update their beliefs about $\\theta$, because the beta distribution is __conjugate__ to the binomial likelihood as well! However, that is easy for me to say..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The researcher started with a completely uniform _prior belief_ over the voting probability $\\theta$, so the settings for $a$ and $b$ are $a=1$ and $b=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Show that the posterior distribution for a beta prior and binomial likelihood is once more a beta distribution. What is the posterior distribution after combining the prior and the likelihood, for the settings of $a$ and $b$ you motivated in question 1, and the observations as given above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\theta | z, N) = \\frac{p(z | N, \\theta)p(\\theta)}{p(z | N)}$$\n",
    "\n",
    "$$p(z | N,\\theta)p(\\theta) =  \\theta^{z+a-1} (1 - \\theta)^{N-z+b-1} \\frac{1}{B(a,b)} d\\theta$$\n",
    "\n",
    "$$p(z | N) = \\int \\theta^z (1 - \\theta)^{N-z} \\theta^{a-1} (1-\\theta)^{b-1} \\frac{1}{B(a,b)} d\\theta$$\n",
    "\n",
    "$$p(z | N) = \\theta^{z+a-1} (1 - \\theta)^{N-z+b-1} \\frac{1}{B(a+z, b+N-z)}$$\n",
    "\n",
    "$$p(\\theta | z, N) = beta(\\theta | a, b, z, N)$$\n",
    "\n",
    "Given $z=113$, $N=145$, $a=1$ and $b=1$, the posterios distribution is:\n",
    "$$p(\\theta | z, N) = beta(\\theta | a, b, z, N)= beta(\\theta | 1, 1, 113, 145)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Make a plot with $\\theta \\in [0,1]$ on the horizontal axis. Show the prior $p(\\theta\\mid a, b)$, with the settings for $a$ and $b$ as you chose in question 1, as well as the posterior $p(\\theta\\mid a, b, z, N)$ that you determined in question 2. You can use ``scipy.stats.beta.pdf()`` as an easy way to get the beta probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats.beta as beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use the function ``hdi()`` in the code block below to compute the 95% highest posterior density. What is the HDI? Superimpose this interval on your plot using ``plt.axvspan(..., alpha=0.1)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiORqcOPgYT7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hdi(theta, pdf, level=0.95):\n",
    "    sorted_p = np.sort(pdf)[::-1]\n",
    "    HDI_height = min(np.cumsum([np.cumsum(sorted_p) >= level]))\n",
    "    HDI = theta[pdf >= HDI_height]\n",
    "    # Return bounds of the HDI:\n",
    "    return HDI[0], HDI[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFrbZiitgYT9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A year later, the researcher repeats the experiment. But rather than starting with a uniform prior belief again, they use the results from the previous experiment as an _informed prior_. This second experiments results in $z_2 = 84, N_2 = 129$.\n",
    "\n",
    "5. Use the posterior as prior for this new experiment. What is the new posterior when taking your initial prior, the previous experiment, and the current experiment into account?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. Plot the new posterior on top of your previous figure. What is the new 95% HDI? Is this smaller or wider than the previous one? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw1P03rwgYT_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## More on conjugacy\n",
    "\n",
    "Consider once more our favorite theorem:\n",
    "\n",
    "$$ p(\\theta\\mid D) = \\frac{p(D\\mid \\theta)p(\\theta)}{p(D)} \\enspace$$\n",
    "\n",
    "As we saw in the lecture, if the prior $p(\\theta)$ is conjugate to the likelihood $p(D\\mid\\theta)$, we are in luck: in that case the posterior $p(\\theta\\mid D)$ has the same form as the prior, just with updated parameters. But that is not all! In the conjugate case, we also have an analytic solution for the _evidence_ (a.k.a. _marginal likelihood_) $P(D)=\\int p(D\\mid \\theta)p(\\theta) \\text{d}\\theta$. That means if we want to do something with $p(D)$, we do not have to use a pesky integral (and later in the course we will see that $p(D)$ has important uses).\n",
    "\n",
    "Let's assume once more the binomial likelihood:\n",
    "\n",
    "$$ p(z\\mid \\theta, N) = \\binom{N}{z} \\theta^{z} (1-\\theta)^{N-z} $$\n",
    "\n",
    "and the beta prior:\n",
    "\n",
    "$$ p(\\theta \\mid a, b) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1})}{B(a,b)} \\enspace.$$\n",
    "\n",
    "Prove that\n",
    "$$ p(z\\mid N, a,b) = \\binom{N}{z}\\frac{B(z+a, N-z+b)}{B(a,b)} \\enspace.$$\n",
    "\n",
    "Note: if you are using a lot of calculus to get to the result, you are doing it wrong! You only have to expand the expressions for prior and likelihood and look carefully for terms that you recognize / know the solution for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0pMfXMygYUA"
   },
   "source": [
    "## Conjugacy with other distributions\n",
    "\n",
    "So far we have looked at the beta distribution, as well as the binomial and product-of-Bernoullis (the latter two being very similar). But of course, conjugacy is not restricted to these distributions -- there are many more examples.\n",
    "\n",
    "For this exercise, we'll consider the *Poisson distribution*. It describes how often some rare event occurred within a specified time frame (e.g., the number of days of rain in the Sahara dessert, in the span of a year). The Poisson distribution assigns a probability to an observation $k$ (a discrete number) given parameter $\\lambda$ (a continuous number, indicating the rate at which these events happen) according to the following probability distribution:\n",
    "\n",
    "$$ p(k \\mid  \\lambda) = \\mathrm{Poisson}(k\\mid \\lambda)= \\frac{e^{-\\lambda} \\lambda^k}{k!} \\enspace. $$\n",
    "\n",
    "This is important: $k$ is observed, $\\lambda$ is a parameter that we wish to learn, using that data. The parameter tells us for example what the rate of rain in the Sahara is.\n",
    "\n",
    "1. For this question, let's pick $\\lambda=5$. Make a plot of the probability distribution $p(k\\mid \\lambda=5)$ for $k=0,\\ldots, 20$. Remember that $k$ is integer here, so it makes more sense to use ``plt.bar()`` than ``plt.plot()`` as you did for continuous numbers. You can use ``scipy.stats.poisson.pmf`` to easily compute the Poisson probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ6mKnHDgYUA"
   },
   "source": [
    "2. If you sum the probabilities you computed in the previous question over the different values for $k$, the result is (approximately) 1. Why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFnWOSm-gYUA"
   },
   "source": [
    "Of course, there exists a probability distribution that is conjugate to the Poisson distribution. This one is known as the _gamma_ distribution. It has this form:\n",
    "\n",
    "$$ p(\\lambda \\mid  \\alpha, \\beta) = \\mathrm{Gamma}(\\lambda\\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta\\lambda} \\enspace, $$\n",
    "\n",
    "with two *hyperparameters*, $\\alpha$ and $\\beta$.\n",
    "\n",
    "3. Which part of this probability density function is the normalization, i.e. the part that ensures the distribution integrates to 1?\n",
    "\n",
    "4. Show that indeed the gamma prior is conjugate to the Poisson likelihood. That is, compute the posterior and show that it has exactly the gamma distribution form, but that $\\alpha$ and $\\beta$ have been updated.\n",
    "\n",
    "Now let's set $\\alpha=5$ and $\\beta=2$ to determine our prior belief about $\\lambda$. Furthermore, assume we observe $k=8$.\n",
    "\n",
    "5. Make a figure that shows (in three different ``plt.subplots(nrows=1, ncols=3, sharex=True)`` the prior in the left subplot, the likelihood in the middle subplot, and the posterior in the right subplot. All plots should have $\\lambda \\in [0, 20]$ on the horizontal axis. Since $\\lambda$ is a continuous variable, it makes sense to use ``plt.plot()`` again (or ``ax.plot()`` since we are working with subplots, see below). You can use ``scipy.stats.gamma.pdf()`` to compute the gamma probability density function. Note: call the SciPy gamma function as: ``scipy.stats.gamma.pdf(lambda, alpha, scale=1/beta)`` or your results will not make sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMsY6AmkgYUC",
    "outputId": "c0e93297-c137-4ab7-b1b7-145fc79252dc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gamma, poisson\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lambda_range = np.linspace(0, 20, num=100)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True)\n",
    "# axes[0].plot(...)  # show prior\n",
    "# axes[1].plot(...)  # show likelihood\n",
    "# axes[2].plot(...)  # show posterior\n",
    "# add appropriate labels and limits to your figure!\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__gsNnAJgYUD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6. Compute the 95% HDI of $\\lambda$ for the prior and for the posterior and superimpose it on the corresponding plots using ``ax.axvspan()``. How has it changed?\n",
    "\n",
    "7. Why does the likelihood in the middle plot __not__ integrate to 1?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
